# DataFlow-Automator with Kafka & Spark

## Project Overview
**DataFlow-Automator** is an end-to-end data engineering project that implements real-time data ingestion, processing, and orchestration using Kafka and Spark. The project aims to automate data workflows, enabling efficient data handling and analysis.

## Key Features
- **Kafka Integration**: Stream real-time data into the pipeline.
- **Spark Processing**: Utilize Apache Spark for large-scale data processing and analytics.
- **Dockerized Environment**: Easily deploy and manage the application using Docker.
- **Airflow DAGs**: Orchestrate data workflows for seamless execution.

## Technologies Used
- **Python**: Primary programming language for the project.
- **Apache Kafka**: For real-time data streaming.
- **Apache Spark**: For data processing and analytics.
- **Docker**: For containerization and easy deployment.
- **Apache Airflow**: For orchestrating data workflows.

## Installation
1. Clone the repository:
   ```bash
   git clone https://github.com/SahilTalathi142001/DataFlow-Automator-with-Kafka-Spark.git
   cd DataFlow-Automator-with-Kafka-Spark

2. Set up a virtual environment:

python -m venv .venv
source .venv/bin/activate   # On Windows use `.venv\Scripts\activate`

3. Run the application using Docker:

docker compose up -d


